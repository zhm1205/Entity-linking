{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e07bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "from google.colab import drive\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Data loading and positive/negative datasets construction\n",
    "def load_json_gz(filepath):\n",
    "    \"\"\"Loads a gzipped json file.\"\"\"\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_entity_linking_samples(medmentions_data, umls_data, max_negative_per_positive=2, sample_size=100):\n",
    "    \"\"\"\n",
    "    Generates positive and negative samples for entity linking with balanced sampling.\n",
    "\n",
    "    Args:\n",
    "    medmentions_data: A list of MedMentions data dictionaries.\n",
    "    umls_data: A list of UMLS data dictionaries.\n",
    "    max_negative_per_positive: Maximum number of negative samples to generate for each positive sample.\n",
    "    sample_size: The number of MedMentions entries to use.\n",
    "\n",
    "    Returns:\n",
    "    A list of dictionaries, each representing a sample with \"text\", \"label\".\n",
    "    \"\"\"\n",
    "\n",
    "    umls_dict = {entry['cui']: entry for entry in umls_data if 'cui' in entry and 'name' in entry}\n",
    "    samples = []\n",
    "\n",
    "    # Sample MedMentions data\n",
    "    sampled_medmentions_data = random.sample(medmentions_data, min(sample_size, len(medmentions_data)))\n",
    "\n",
    "\n",
    "    for doc in sampled_medmentions_data:\n",
    "        if 'annotations' in doc:\n",
    "            for annotation in doc['annotations']:\n",
    "              if 'cui' in annotation and  annotation['cui'] in umls_dict and 'text' in annotation:\n",
    "\n",
    "                # positive samples\n",
    "                positive_samples = {\n",
    "                        \"text\": f\"{annotation['text']} [SEP] {umls_dict[annotation['cui']]['name']}\",\n",
    "                        \"label\": 1\n",
    "                   }\n",
    "                samples.append(positive_samples)\n",
    "                # negative samples\n",
    "                negative_count = 0\n",
    "                \n",
    "                while negative_count < max_negative_per_positive:\n",
    "                    \n",
    "                   negative_cui = random.choice([c for c in umls_dict.keys() if c != annotation['cui']])\n",
    "                   negative_samples = {\n",
    "                      \"text\": f\"{annotation['text']} [SEP] {umls_dict[negative_cui]['name']}\",\n",
    "                       \"label\": 0\n",
    "                   }\n",
    "                   samples.append(negative_samples)\n",
    "                   negative_count +=1\n",
    "\n",
    "\n",
    "    # Balance the dataset, ensure 50/50 positive/negative\n",
    "    positive_samples = [sample for sample in samples if sample['label'] == 1]\n",
    "    negative_samples = [sample for sample in samples if sample['label'] == 0]\n",
    "    \n",
    "    num_pos = len(positive_samples)\n",
    "    num_neg = len(negative_samples)\n",
    "    \n",
    "    if num_pos > num_neg:\n",
    "       sampled_positive = random.sample(positive_samples, num_neg)\n",
    "       final_samples = sampled_positive + negative_samples\n",
    "    elif num_neg > num_pos:\n",
    "      sampled_negative = random.sample(negative_samples, num_pos)\n",
    "      final_samples = sampled_negative + positive_samples\n",
    "    else:\n",
    "       final_samples = samples\n",
    "    \n",
    "\n",
    "    return final_samples\n",
    "\n",
    "\"\"\"# load MedMentions dataset\n",
    "def load_medmentions(file_path):\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# load UMLS dataset\n",
    "def load_umls(file_path):\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "medmentions_data = load_medmentions(medmentions_path)\n",
    "umls_data = load_umls(umls_path)\n",
    "\n",
    "# create index of UMLS \n",
    "umls_index = {entry['cui']: entry for entry in umls_data}\n",
    "\n",
    "# construct positive and negative samples\n",
    "positive_examples = []\n",
    "negative_examples = []\n",
    "\n",
    "for doc in medmentions_data[:10]:\n",
    "    if 'annotations' in doc:\n",
    "        for annotation in doc['annotations']:\n",
    "            if 'cui' in annotation and annotation['cui'] in umls_index:\n",
    "                # 正例\n",
    "                positive_examples.append({\n",
    "                    \"text\": f\"{annotation['text']} [SEP] {umls_index[annotation['cui']]['name']}\",\n",
    "                    \"label\": 1\n",
    "                })\n",
    "                # 负例\n",
    "                random_cui = random.choice(list(umls_index.keys()))\n",
    "                while random_cui == annotation['cui']:\n",
    "                  random_cui = random.choice(list(umls_index.keys()))\n",
    "\n",
    "                  #random_cui = random.choice([c for c in umls_index.keys() if c != annotation['cui']])\n",
    "                negative_examples.append({\n",
    "                    \"text\": f\"{annotation['text']} [SEP] {umls_index[random_cui]['name']}\",\n",
    "                    \"label\": 0\n",
    "                })\n",
    "\n",
    "all_examples = positive_examples + negative_examples\n",
    "random.shuffle(all_examples)\n",
    "df = pd.DataFrame(all_examples)\n",
    "print(df)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f44749",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Assume the files are in the \"colab\" directory in Google Drive\n",
    "    medmentions_filepath = \"/content/drive/My Drive/Colab/medmentions.json.gz\"\n",
    "    umls_filepath = \"/content/drive/My Drive/Colab/umls.json.gz\"\n",
    "\n",
    "    # Load data from the files\n",
    "    try:\n",
    "        medmentions_data = load_json_gz(medmentions_filepath)\n",
    "        umls_data = load_json_gz(umls_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find either '{medmentions_filepath}' or '{umls_filepath}'. Please make sure these files are in the specified directory on Google Drive.\")\n",
    "        samples = [] # return an empty list\n",
    "        exit()\n",
    "    else: # only create samples if no error\n",
    "        # Create samples\n",
    "        samples = create_entity_linking_samples(medmentions_data, umls_data, sample_size=100, max_negative_per_positive=2)\n",
    "\n",
    "        df = pd.DataFrame(samples)\n",
    "        df = df.sample(frac=1).reset_index(drop=True) # shuffle the dataframe\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    # Split dataset\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        df['text'], df['label'], test_size=0.3, random_state=42\n",
    "    )\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # Transfer to HuggingFace data structure\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels.tolist()})\n",
    "    val_dataset = Dataset.from_dict({\"text\": val_texts.tolist(), \"label\": val_labels.tolist()})\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"label\": test_labels.tolist()})\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    model_name = \"allenai/biomed_roberta_base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # HuggingFace Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    trainer.train()\n",
    "    print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    # Use Trainer to evaluate test datasets\n",
    "    test_results = trainer.predict(tokenized_test)\n",
    "    metrics = compute_metrics((test_results.predictions, test_results.label_ids))\n",
    "    print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
